{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral image ground classification\n",
    "## Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:41.088513Z",
     "start_time": "2018-12-17T15:11:37.785051Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meryll/miniconda3/envs/eca/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Activation, Average, Dense, Flatten\n",
    "from keras.layers.convolutional import AveragePooling3D, Conv3D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import itertools\n",
    "import spectral\n",
    "from spectral import spy_colors\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:41.128448Z",
     "start_time": "2018-12-17T15:11:41.091961Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Function found on stackoverflow\n",
    "def printProgressBar (iteration, total, prefix = 'Progress: ', suffix = ' Complete', decimals = 1, length = 40, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    sys.stdout.flush()\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "def print_shape(**kwargs):\n",
    "    \"\"\"\n",
    "    Print multiple shapes of np.ndarray\n",
    "    \"\"\"\n",
    "    for key, value in kwargs.items():\n",
    "        print (\"%s: %s\" %(key, value.shape))\n",
    "        \n",
    "def loadData():\n",
    "    data_path = os.path.join(os.getcwd(),'.')\n",
    "    data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
    "    train_labels = np.load(\"train_data.npy\")\n",
    "    test_labels = np.load(\"test_data.npy\")\n",
    "    \n",
    "    return data, train_labels, test_labels\n",
    "\n",
    "def displayPrincipalComponents(X, cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Display principal components of the sensor data array\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    cmap : str, optional\n",
    "        Custom color map for matplotlib\n",
    "    \"\"\"\n",
    "    pc = spectral.principal_components(X_train)\n",
    "    plt.figure()\n",
    "    plt.imshow(pc.cov, cmap=cmap)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def reduceComponents(X, reduce_factor=7):\n",
    "    \"\"\"\n",
    "    Reduce aviris sensor data array in principal components\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    reduce_factor : int, optional\n",
    "        Determines the strength of dimensionality reduction\n",
    "    \"\"\"\n",
    "    switcher = {\n",
    "        1: 0.9,\n",
    "        2: 0.99,\n",
    "        3: 0.999,\n",
    "        4: 0.9999,\n",
    "        5: 0.99999,\n",
    "        6: 0.999999,\n",
    "        7: 0.9999999\n",
    "    }\n",
    "    fraction = switcher.get(reduce_factor, 7)\n",
    "    pc = spectral.principal_components(X).reduce(fraction=fraction)\n",
    "\n",
    "    # How many eigenvalues are left?\n",
    "\n",
    "    print(\"Reflectance bands remaining: %s\" %(len(pc.eigenvalues)))\n",
    "    newX = pc.transform(X)\n",
    "\n",
    "    #v = plt.imshow(img_pc[:,:,1], cmap=\"cool\")\n",
    "    return newX\n",
    "\n",
    "def displayImage(X, img_num=3, cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Display image from sensor data array\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    img_num : int, optional\n",
    "        Display band 'img_num'\n",
    "    cmap : str, optional\n",
    "        Custom color map for matplotlib\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(X[:,:,img_num], cmap=cmap)\n",
    "    \n",
    "def patch_1dim_split(X, train_data, test_data, PATCH_SIZE):\n",
    "    padding = int((PATCH_SIZE - 1) / 2) #Patch de 3*3 = padding de 1 (centre + 1 de chaque coté)\n",
    "    #X_padding = np.zeros(X)\n",
    "    X_padding = np.pad(X, [(padding, padding), (padding, padding), (0, 0)], mode='constant')\n",
    "    \n",
    "    X_patch = np.zeros((X.shape[0] * X.shape[1], PATCH_SIZE, PATCH_SIZE, X.shape[2]))\n",
    "    y_train_patch = np.zeros((train_data.shape[0] * train_data.shape[1]))\n",
    "    y_test_patch = np.zeros((test_data.shape[0] * test_data.shape[1]))\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(0, X_padding.shape[0] - 2 * padding):\n",
    "        for j in range(0, X_padding.shape[1] - 2 * padding):\n",
    "            # This condition is for less frequent updates. \n",
    "            if i % 8 == 0 or index == (X_padding.shape[0] - 2 * padding) * (X_padding.shape[1] - 2 * padding) - 1:\n",
    "                printProgressBar(index + 1, (X_padding.shape[0] - 2 * padding) * (X_padding.shape[1] - 2 * padding))\n",
    "            patch = X_padding[i:i + 2 * padding + 1, j:j + 2 * padding + 1]\n",
    "            X_patch[index, :, :, :] = patch\n",
    "            y_train_patch[index] = train_data[i, j]\n",
    "            y_test_patch[index] = test_data[i, j]\n",
    "            index += 1\n",
    "    \n",
    "    print(\"\\nCreating train/test arrays and removing zero labels...\")\n",
    "    printProgressBar(1, 7)\n",
    "    X_train_patch = np.copy(X_patch)\n",
    "    printProgressBar(2, 7)\n",
    "    X_test_patch = np.copy(X_patch)\n",
    "    \n",
    "    printProgressBar(3, 7)\n",
    "    X_train_patch = X_train_patch[y_train_patch > 0,:,:,:]\n",
    "    printProgressBar(4, 7)\n",
    "    X_test_patch = X_test_patch[y_test_patch > 0,:,:,:]\n",
    "    printProgressBar(5, 7)\n",
    "    y_train_patch = y_train_patch[y_train_patch > 0] - 1\n",
    "    printProgressBar(6, 70)\n",
    "    y_test_patch = y_test_patch[y_test_patch > 0] - 1\n",
    "    printProgressBar(7, 7)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return X_train_patch, X_test_patch, y_train_patch, y_test_patch\n",
    "\n",
    "def dimensionalityReduction(X, numComponents=75, standardize=True):\n",
    "    if standardize:\n",
    "        newX = np.reshape(X, (-1, X.shape[2]))\n",
    "        scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "        newX = scaler.transform(newX)\n",
    "        X = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n",
    "    \n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "\n",
    "def BoostDataset(X, y, n_samples=0):\n",
    "    # Techniques from \n",
    "    # https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec\n",
    "    \n",
    "    orig_shape = X.shape[0]\n",
    "    index = orig_shape\n",
    "    print(\"Boosting Dataset...\")\n",
    "    for i in range(n_samples):\n",
    "        if i % 5 == 0 or i + 1 == n_samples:\n",
    "            printProgressBar(i + 1, n_samples)\n",
    "        num_sample = random.randint(0, orig_shape)\n",
    "        patch = X[num_sample,:,:,:]\n",
    "        #print(patch.shape)\n",
    "        num = random.randint(0, 4)\n",
    "        if (num == 0):\n",
    "            new_patch = np.flipud(patch)\n",
    "            \n",
    "        if (num == 1):\n",
    "            new_patch = np.fliplr(patch)\n",
    "            \n",
    "        if (num == 2):\n",
    "            new_patch = sk.util.random_noise(patch)\n",
    "            \n",
    "        if (num == 3 or num == 4):\n",
    "            random_degree = random.uniform(-25, 25)\n",
    "            new_patch = sk.transform.rotate(patch, random_degree)\n",
    "            \n",
    "        #print(new_patch.shape)\n",
    "        #time.sleep(5)\n",
    "            \n",
    "        X = np.append(X, [new_patch], axis=0)\n",
    "        y = np.append(y, y[num_sample])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def reports (X_test,y_test, target_names):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    Test_Loss =  score[0]*100\n",
    "    Test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, Test_Loss, Test_accuracy\n",
    "\n",
    "def Patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
    "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch\n",
    "\n",
    "\n",
    "\n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts  \n",
    "    # repeat for every label and concat\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y == label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:41.165915Z",
     "start_time": "2018-12-17T15:11:41.131552Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "numPCAcomponents = 30\n",
    "PATCH_SIZE = 5\n",
    "C1 = 3*numPCAcomponents\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "target_names = ['Maïs- Pas de Technique de Conservation des sols',\n",
    "                    'Maïs- Minimum Tillage',\n",
    "                    'Herbe-Pâturage',\n",
    "                    'Herbe-Arbre',\n",
    "                    'Soja-Pas de Technique de Conservation des sols', 'Soja-Minimum Tillage','Soja', 'Bois',\n",
    "                    'Bâtiment-Herbe-Arbre-drives',]\n",
    "\n",
    "label_dictionary = {\n",
    "    0: 'Rien',\n",
    "    1: 'Maïs- Pas de Technique de Conservation des sols', \n",
    "    2: 'Maïs- Minimum Tillage',\n",
    "    3: 'Herbe-Pâturage', \n",
    "    4: 'Herbe-Arbre', \n",
    "    5: 'Soja-Pas de Technique de Conservation des sols', \n",
    "    6: 'Soja-Minimum Tillage',\n",
    "    7: 'Soja', \n",
    "    8: 'Bois', \n",
    "    9: 'Bâtiment-Herbe-Arbre-drives',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:41.257887Z",
     "start_time": "2018-12-17T15:11:41.167940Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X, train_data, test_data = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:18:12.648353Z",
     "start_time": "2018-11-25T14:18:12.639908Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is essential to insure a random distribution of classes between the test / train data\n",
    "\n",
    "def shuffleTrainTest(train, test):\n",
    "    np.random.seed(42)\n",
    "    for i in range(train.shape[0]):\n",
    "        for j in range(train.shape[1]):\n",
    "            if train[i, j] != 0 or test[i, j] != 0 : #eviter calcul inutiles\n",
    "                x = np.random.randint(1,3)\n",
    "                if x == 1:\n",
    "                    temp = train[i, j]\n",
    "                    train[i, j] = test[i, j]\n",
    "                    test[i, j] = temp\n",
    "    return train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.531420Z",
     "start_time": "2018-12-17T15:11:41.260029Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  5  6 10 11 12 14 15] [15155   648   751   465   492   231  1646   403  1145    89]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16] [16646    46   780    79   237    18   238    28   478    20   741   809\n",
      "   190   205   120   297    93]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shuffleTrainTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eb030d32fc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeleteUselessClasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_authorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeleteUselessClasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_authorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffleTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffleTrainTest' is not defined"
     ]
    }
   ],
   "source": [
    "# Check random distribution and delete classes\n",
    "t,v = np.unique(train_data, return_counts=True)\n",
    "print(t, v)\n",
    "t,v = np.unique(test_data, return_counts=True)\n",
    "print(t, v)\n",
    "\n",
    "classes_authorized = [2,3,5,6,10,0,11,12,14,15]\n",
    "\n",
    "def deleteUselessClasses(data, classes_authorized):\n",
    "    #data = data[data.any() in classes_authorized]\n",
    "    #if not data\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if data[i][j] not in classes_authorized:\n",
    "                data[i][j] = 0\n",
    "            if data[i][j] == 2:\n",
    "                data[i][j] = 1\n",
    "            if data[i][j] == 3:\n",
    "                data[i][j] = 2\n",
    "            if data[i][j] == 5:\n",
    "                data[i][j] = 3\n",
    "            if data[i][j] == 6:\n",
    "                data[i][j] = 4\n",
    "            if data[i][j] == 10:\n",
    "                data[i][j] = 5\n",
    "            if data[i][j] == 11:\n",
    "                data[i][j] = 6\n",
    "            if data[i][j] == 12:\n",
    "                data[i][j] = 7\n",
    "            if data[i][j] == 14:\n",
    "                data[i][j] = 8\n",
    "            if data[i][j] == 15:\n",
    "                data[i][j] = 9\n",
    "    return data\n",
    "\n",
    "train_data = deleteUselessClasses(train_data, classes_authorized)\n",
    "test_data = deleteUselessClasses(test_data, classes_authorized)\n",
    "train_data, test_data = shuffleTrainTest(train_data, test_data)\n",
    "t,v = np.unique(train_data, return_counts=True)\n",
    "print(t, v)\n",
    "t,v = np.unique(test_data, return_counts=True)\n",
    "print(t, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.532516Z",
     "start_time": "2018-12-17T15:11:37.791Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X, pca = dimensionalityReduction(X, numComponents=numPCAcomponents, standardize=False)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.533829Z",
     "start_time": "2018-12-17T15:11:37.792Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Add padding, move patch along the array, make image one dimensional, and split provided train and test_data with removing zeros labels at the same time\n",
    "# X.shape : (Pixels_in_one_band, patch_row, patch_col, reflectance_band) \n",
    "# Pixels_in_one_band.shape = 145 * 145 = 21025\n",
    "\n",
    "X_train, X_test, y_train, y_test = patch_1dim_split(X, train_data, test_data, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.535434Z",
     "start_time": "2018-12-17T15:11:37.793Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = oversampleWeakClasses(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:21:47.608186Z",
     "start_time": "2018-11-25T14:21:14.905243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add samples. Warning: 33.3s for 100 samples on my PC if no dimensionality reduction. There's room for optimization here. \n",
    "# Optimization Tip: Array is recreated at each iteration.\n",
    "X_train, y_train = BoostDataset(X_train, y_train, n_samples=1000)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.537159Z",
     "start_time": "2018-12-17T15:11:37.795Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if X and y have compatible shapes (X_*.shape[0] === y_*.shape[0])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_test, y_test = oversampleWeakClasses(X_test, y_test)\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.538257Z",
     "start_time": "2018-12-17T15:11:37.799Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[3], X_train.shape[1], X_train.shape[2]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[3], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=9)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:11:42.539482Z",
     "start_time": "2018-12-17T15:11:37.801Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "input_shape= X_train[0].shape\n",
    "print(input_shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:24:25.668642Z",
     "start_time": "2018-11-25T14:24:18.516484Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequential_cnn_model(input_shape, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(C1, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(3*C1, (3,3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30*numPCAcomponents, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "sequential_cnn_model = sequential_cnn_model(input_shape)\n",
    "\n",
    "# On a 9 classes en réalité, donc une accuracy > 100/9 ~=11.11 est supérieure au hasard. \n",
    "sequential_cnn_model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=7,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T02:52:54.411669Z",
     "start_time": "2018-11-19T02:52:52.954413Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Reshape\n",
    "\n",
    "def bn_prelu(input):\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    # return Activation(\"relu\")(norm)\n",
    "    return PReLU()(norm)\n",
    "\n",
    "\n",
    "def spectral_conv(input):\n",
    "    activation = bn_prelu(input)\n",
    "    conv = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 1), kernel_regularizer=l2(0.0001),\n",
    "                  filters=growth_rate, kernel_size=(1, 1, 7), padding='same', dilation_rate=(1, 1, 1))(activation)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def spatial_conv(input):\n",
    "    activation = bn_prelu(input)\n",
    "    conv = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 1), kernel_regularizer=l2(0.0001),\n",
    "                  filters=growth_rate, kernel_size=(3, 3, 1), padding='same', dilation_rate=(1, 1, 1))(activation)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global CONV_DIM1\n",
    "    global CONV_DIM2\n",
    "    global CONV_DIM3\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        CONV_DIM1 = 1\n",
    "        CONV_DIM2 = 2\n",
    "        CONV_DIM3 = 3\n",
    "        CHANNEL_AXIS = 4\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        CONV_DIM1 = 2\n",
    "        CONV_DIM2 = 3\n",
    "        CONV_DIM3 = 4\n",
    "\n",
    "\n",
    "class fdssc_model(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        global growth_rate\n",
    "        growth_rate = 12\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 4:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2, kernel_dim3)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[3], input_shape[0])\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        print(\"the dim of input:\", input._keras_shape[3])\n",
    "        # Dense spectral block\n",
    "\n",
    "        x1_0 = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 2), kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      filters=24, kernel_size=(1, 1, 7), padding='valid')(input)\n",
    "        x1_1 = spectral_conv(x1_0)\n",
    "        x1_1_ = concatenate([x1_0, x1_1], axis=CHANNEL_AXIS)\n",
    "        x1_2 = spectral_conv(x1_1_)\n",
    "        x1_2_ = concatenate([x1_0, x1_1, x1_2], axis=CHANNEL_AXIS)\n",
    "        x1_3 = spectral_conv(x1_2_)\n",
    "        x1 = concatenate([x1_0, x1_1, x1_2, x1_3], axis=CHANNEL_AXIS)\n",
    "        x1 = bn_prelu(x1)\n",
    "\n",
    "        print('the output of dense spectral block:', x1._keras_shape)\n",
    "\n",
    "        # Reducing dimension layer\n",
    "        # x1 = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 1), kernel_regularizer=regularizers.l2(0.0001),\n",
    "        #            filters=24, kernel_size=(1, 1, 1), padding='valid')(x1)\n",
    "        tran1 = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 1), kernel_regularizer=regularizers.l2(0.0001),\n",
    "                       filters=200, kernel_size=(1, 1, x1._keras_shape[CONV_DIM3]), padding='valid')(x1)\n",
    "        print(tran1._keras_shape)\n",
    "        tran1 = bn_prelu(tran1)\n",
    "        tran2 = Reshape((tran1._keras_shape[CONV_DIM1], tran1._keras_shape[CONV_DIM2],\n",
    "                         tran1._keras_shape[CHANNEL_AXIS], 1))(tran1)\n",
    "\n",
    "        x2_0 = Conv3D(kernel_initializer='he_normal', strides=(1, 1, 1), kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      filters=24, kernel_size=(3, 3, 200), padding='valid')(tran2)\n",
    "        print('the input of dense spatial block:', x2_0._keras_shape)\n",
    "\n",
    "        # Dense spatial block\n",
    "        x2_1 = spatial_conv(x2_0)\n",
    "        x2_1_ = concatenate([x2_0, x2_1], axis=CHANNEL_AXIS)\n",
    "        x2_2 = spatial_conv(x2_1_)\n",
    "        x2_2_ = concatenate([x2_0, x2_1, x2_2], axis=CHANNEL_AXIS)\n",
    "        x2_3 = spatial_conv(x2_2_)\n",
    "        x2 = concatenate([x2_0, x2_1, x2_2, x2_3], axis=CHANNEL_AXIS)\n",
    "\n",
    "        print('the output of dense spectral block is:', x2._keras_shape)\n",
    "        x2 = bn_prelu(x2)\n",
    "\n",
    "        # Classifier block\n",
    "        pool1 = AveragePooling3D(pool_size=(x2._keras_shape[1], x2._keras_shape[2], 1), strides=(1, 1, 1))(x2)\n",
    "\n",
    "        flatten1 = Flatten()(pool1)\n",
    "        drop1 = Dropout(0.5)(flatten1)\n",
    "        dense = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"glorot_normal\")(drop1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_fdssc(input_shape, num_outputs):\n",
    "        return fdssc_model.build(input_shape, num_outputs)\n",
    "\n",
    "model = fdssc_model.build_fdssc((1, 5, 5, 30), 9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T03:08:23.976983Z",
     "start_time": "2018-11-19T03:08:17.216137Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1), y_train,\n",
    "        batch_size=batch_size, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:04.086421Z",
     "start_time": "2018-11-18T17:48:04.034793Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def nin_cnn():\n",
    "    model = Sequential()\n",
    "    #mlpconv block 1\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu',padding='valid', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((1,1), dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block2\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((1,1), dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block3\n",
    "    model.add(Conv2D(128, (1, 1), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(16, (1, 1)))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation(activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "nin_cnn = nin_cnn()\n",
    "nin_cnn.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:24:38.569232Z",
     "start_time": "2018-11-25T14:24:38.565616Z"
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate_error(model):\n",
    "#     pred = model.predict(X_test, batch_size = 32)\n",
    "#     pred = np.argmax(pred, axis=1)\n",
    "#     pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "#     error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]    \n",
    "#     return error\n",
    "\n",
    "# def ensemble(models, model_input):\n",
    "    \n",
    "#     outputs = [model.outputs[0] for model in models]\n",
    "#     y = Average()(outputs)\n",
    "#     model = Model(model_input, y, name='ensemble')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# models = [nin_cnn, sequential_cnn_model]\n",
    "\n",
    "# ensemble_model = ensemble(models, input_shape)\n",
    "\n",
    "# print(evaluate_error(sequential_cnn_model))\n",
    "# print(evaluate_error(nin_cnn))\n",
    "model = sequential_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T15:30:26.196422Z",
     "start_time": "2018-11-18T15:30:26.163395Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# mlb = MultiLabelBinarizer(classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "# y_enc = mlb.fit_transform(y_train)\n",
    "# clf2 = LogisticRegression()\n",
    "# print(y_enc)\n",
    "# voting = VotingClassifier(estimators=[('cnn', model), ('lr', clf2)], weights=[1.0, 1.0])\n",
    "# voting.fit(X_train, y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:24:46.260312Z",
     "start_time": "2018-11-25T14:24:45.196336Z"
    }
   },
   "outputs": [],
   "source": [
    "classification, confusion, Test_loss, Test_accuracy = reports(X_test,y_test, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:24:49.396283Z",
     "start_time": "2018-11-25T14:24:49.390771Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:25:01.224421Z",
     "start_time": "2018-11-25T14:25:00.605438Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "plot_confusion_matrix(confusion, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:27:28.752823Z",
     "start_time": "2018-11-25T14:27:28.219832Z"
    }
   },
   "outputs": [],
   "source": [
    "X_garbage, train_data, test_data = loadData()\n",
    "#X = dimensionalityReduction(X, )\n",
    "y = np.add(train_data, test_data)\n",
    "y = deleteUselessClasses(y, classes_authorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:27:47.188894Z",
     "start_time": "2018-11-25T14:27:29.077067Z"
    }
   },
   "outputs": [],
   "source": [
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "print(width, height)\n",
    "\n",
    "# calculate the predicted image\n",
    "outputs = np.zeros((height,width)) # zeroed image\n",
    "index = 0\n",
    "for i in range(0, height-PATCH_SIZE+1):\n",
    "    if i % 8 == 0 or index == (height-PATCH_SIZE+1) - 1:\n",
    "        printProgressBar(index + 1, (height-PATCH_SIZE+1) )\n",
    "    index += 1\n",
    "    for j in range(0, width-PATCH_SIZE+1):\n",
    "        target = int(y[int(i+PATCH_SIZE/2)][int(j+PATCH_SIZE/2)])\n",
    "        if target == 0 :\n",
    "            continue\n",
    "        else :\n",
    "            image_patch=Patch(X,i,j)\n",
    "            #print (image_patch.shape)\n",
    "            X_test_image = image_patch.reshape(1, image_patch.shape[0],image_patch.shape[1], image_patch.shape[2]).astype('float32')#.reshape(1,image_patch.shape[2],image_patch.shape[0],image_patch.shape[1]).astype('float32')                                   \n",
    "            prediction = (model.predict_classes(X_test_image))                         \n",
    "            outputs[int(i+PATCH_SIZE/2)][int(j+PATCH_SIZE/2)] = prediction + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:27:49.570462Z",
     "start_time": "2018-11-25T14:27:49.339242Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))\n",
    "print(np.unique(y))\n",
    "\n",
    "labelPatches = [ patches.Patch(color=spy_colors[x]/255.,\n",
    "                 label=label_dictionary[x]) for x in np.unique(y) ]\n",
    "plt.legend(handles=labelPatches, ncol=2, fontsize='medium', \n",
    "           loc='upper center', bbox_to_anchor=(0.5, -0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T14:27:50.430468Z",
     "start_time": "2018-11-25T14:27:50.295157Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
