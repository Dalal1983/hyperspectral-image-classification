{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral image ground classification\n",
    "## Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:53.649003Z",
     "start_time": "2018-11-18T17:46:53.638579Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import itertools\n",
    "import spectral\n",
    "from spectral import spy_colors\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:53.685194Z",
     "start_time": "2018-11-18T17:46:53.651513Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "numPCAcomponents = 50\n",
    "PATCH_SIZE = 3\n",
    "C1 = 3*numPCAcomponents\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "label_dictionary = {\n",
    "    0: 'Rien',\n",
    "    1: 'Luzerne', \n",
    "    2: 'Maïs- Pas de Technique de Conservation des sols', \n",
    "    3: 'Maïs- Minimum Tillage', \n",
    "    4: 'Maïs',\n",
    "    5: 'Herbe-Pâturage', \n",
    "    6: 'Herbe-Arbre', \n",
    "    7: 'Herbe-Pâturage-Tondu',\n",
    "    8: 'Foin-andains', \n",
    "    9: 'Avoine', \n",
    "    10: 'Soja-Pas de Technique de Conservation des sols', \n",
    "    11: 'Soja-Minimum Tillage',\n",
    "    12: 'Soja', \n",
    "    13: 'Blé', \n",
    "    14: 'BoisBâtiment-Herbe-Arbre-drives', \n",
    "    15: 'Bâtiment-Herbe-Arbre-drives',\n",
    "    16: 'Pierre-Acier-Tour'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:53.738095Z",
     "start_time": "2018-11-18T17:46:53.688357Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Function found on stackoverflow\n",
    "def printProgressBar (iteration, total, prefix = 'Progress: ', suffix = ' Complete', decimals = 1, length = 40, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    sys.stdout.flush()\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "def print_shape(**kwargs):\n",
    "    \"\"\"\n",
    "    Print multiple shapes of np.ndarray\n",
    "    \"\"\"\n",
    "    for key, value in kwargs.items():\n",
    "        print (\"%s: %s\" %(key, value.shape))\n",
    "        \n",
    "def loadData():\n",
    "    data_path = os.path.join(os.getcwd(),'.')\n",
    "    data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
    "    train_labels = np.load(\"train_data.npy\")\n",
    "    test_labels = np.load(\"test_data.npy\")\n",
    "    \n",
    "    return data, train_labels, test_labels\n",
    "\n",
    "def displayPrincipalComponents(X, cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Display principal components of the sensor data array\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    cmap : str, optional\n",
    "        Custom color map for matplotlib\n",
    "    \"\"\"\n",
    "    pc = spectral.principal_components(X_train)\n",
    "    plt.figure()\n",
    "    plt.imshow(pc.cov, cmap=cmap)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def reduceComponents(X, reduce_factor=7):\n",
    "    \"\"\"\n",
    "    Reduce aviris sensor data array in principal components\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    reduce_factor : int, optional\n",
    "        Determines the strength of dimensionality reduction\n",
    "    \"\"\"\n",
    "    switcher = {\n",
    "        1: 0.9,\n",
    "        2: 0.99,\n",
    "        3: 0.999,\n",
    "        4: 0.9999,\n",
    "        5: 0.99999,\n",
    "        6: 0.999999,\n",
    "        7: 0.9999999\n",
    "    }\n",
    "    fraction = switcher.get(reduce_factor, 7)\n",
    "    pc = spectral.principal_components(X).reduce(fraction=fraction)\n",
    "\n",
    "    # How many eigenvalues are left?\n",
    "\n",
    "    print(\"Reflectance bands remaining: %s\" %(len(pc.eigenvalues)))\n",
    "    newX = pc.transform(X)\n",
    "\n",
    "    #v = plt.imshow(img_pc[:,:,1], cmap=\"cool\")\n",
    "    return newX\n",
    "\n",
    "def displayImage(X, img_num=3, cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Display image from sensor data array\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of dim MxNxP\n",
    "        Sensor data of MxN pixels and P bands\n",
    "    img_num : int, optional\n",
    "        Display band 'img_num'\n",
    "    cmap : str, optional\n",
    "        Custom color map for matplotlib\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(X[:,:,img_num], cmap=cmap)\n",
    "    \n",
    "def patch_1dim_split(X, train_data, test_data, PATCH_SIZE):\n",
    "    padding = int((PATCH_SIZE - 1) / 2) #Patch de 3*3 = padding de 1 (centre + 1 de chaque coté)\n",
    "    #X_padding = np.zeros(X)\n",
    "    X_padding = np.pad(X, [(padding, padding), (padding, padding), (0, 0)], mode='constant')\n",
    "    \n",
    "    X_patch = np.zeros((X.shape[0] * X.shape[1], PATCH_SIZE, PATCH_SIZE, X.shape[2]))\n",
    "    y_train_patch = np.zeros((train_data.shape[0] * train_data.shape[1]))\n",
    "    y_test_patch = np.zeros((test_data.shape[0] * test_data.shape[1]))\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(0, X_padding.shape[0] - 2 * padding):\n",
    "        for j in range(0, X_padding.shape[1] - 2 * padding):\n",
    "            # This condition is for less frequent updates. \n",
    "            if i % 8 == 0 or index == (X_padding.shape[0] - 2 * padding) * (X_padding.shape[1] - 2 * padding) - 1:\n",
    "                printProgressBar(index + 1, (X_padding.shape[0] - 2 * padding) * (X_padding.shape[1] - 2 * padding))\n",
    "            patch = X_padding[i:i + 2 * padding + 1, j:j + 2 * padding + 1]\n",
    "            X_patch[index, :, :, :] = patch\n",
    "            y_train_patch[index] = train_data[i, j]\n",
    "            y_test_patch[index] = test_data[i, j]\n",
    "            index += 1\n",
    "    \n",
    "    print(\"\\nCreating train/test arrays and removing zero labels...\")\n",
    "    printProgressBar(1, 7)\n",
    "    X_train_patch = np.copy(X_patch)\n",
    "    printProgressBar(2, 7)\n",
    "    X_test_patch = np.copy(X_patch)\n",
    "    \n",
    "    printProgressBar(3, 7)\n",
    "    X_train_patch = X_train_patch[y_train_patch > 0,:,:,:]\n",
    "    printProgressBar(4, 7)\n",
    "    X_test_patch = X_test_patch[y_test_patch > 0,:,:,:]\n",
    "    printProgressBar(5, 7)\n",
    "    y_train_patch = y_train_patch[y_train_patch > 0] - 1\n",
    "    printProgressBar(6, 70)\n",
    "    y_test_patch = y_test_patch[y_test_patch > 0] - 1\n",
    "    printProgressBar(7, 7)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return X_train_patch, X_test_patch, y_train_patch, y_test_patch\n",
    "\n",
    "def dimensionalityReduction(X, numComponents=75, standardize=True):\n",
    "    if standardize:\n",
    "        newX = np.reshape(X, (-1, X.shape[2]))\n",
    "        scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "        newX = scaler.transform(newX)\n",
    "        X = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n",
    "    \n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "\n",
    "def BoostDataset(X, y, n_samples=0):\n",
    "    # Techniques from \n",
    "    # https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec\n",
    "    \n",
    "    orig_shape = X.shape[0]\n",
    "    index = orig_shape\n",
    "    print(\"Boosting Dataset...\")\n",
    "    for i in range(n_samples):\n",
    "        if i % 5 == 0 or i + 1 == n_samples:\n",
    "            printProgressBar(i + 1, n_samples)\n",
    "        num_sample = random.randint(0, orig_shape)\n",
    "        patch = X[num_sample,:,:,:]\n",
    "        #print(patch.shape)\n",
    "        num = random.randint(0, 4)\n",
    "        if (num == 0):\n",
    "            new_patch = np.flipud(patch)\n",
    "            \n",
    "        if (num == 1):\n",
    "            new_patch = np.fliplr(patch)\n",
    "            \n",
    "        if (num == 2):\n",
    "            new_patch = sk.util.random_noise(patch)\n",
    "            \n",
    "        if (num == 3 or num == 4):\n",
    "            random_degree = random.uniform(-25, 25)\n",
    "            new_patch = sk.transform.rotate(patch, random_degree)\n",
    "            \n",
    "        #print(new_patch.shape)\n",
    "        #time.sleep(5)\n",
    "            \n",
    "        X = np.append(X, [new_patch], axis=0)\n",
    "        y = np.append(y, y[num_sample])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def reports (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    target_names = ['Luzerne',\n",
    "                    'Maïs- Pas de Technique de Conservation des sols',\n",
    "                    'Maïs- Minimum Tillage',\n",
    "                    'Maïs','Herbe-Pâturage',\n",
    "                    'Herbe-Arbre','Herbe-Pâturage-Tondu','Foin-andains','Avoine',\n",
    "                    'Soja-Pas de Technique de Conservation des sols', 'Soja-Minimum Tillage','Soja',\n",
    "                    'Blé', 'BoisBâtiment-Herbe-Arbre-drives', 'Bâtiment-Herbe-Arbre-drives','Pierre-Acier-Tour']\n",
    "\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    Test_Loss =  score[0]*100\n",
    "    Test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, Test_Loss, Test_accuracy\n",
    "\n",
    "def Patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
    "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch\n",
    "\n",
    "\n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts  \n",
    "    # repeat for every label and concat\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        print(cX)\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:53.834909Z",
     "start_time": "2018-11-18T17:46:53.741821Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X, train_data, test_data = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:54.385476Z",
     "start_time": "2018-11-18T17:46:53.837098Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X, pca = dimensionalityReduction(X, numComponents=numPCAcomponents, standardize=False)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:58.992724Z",
     "start_time": "2018-11-18T17:46:54.387999Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Add padding, move patch along the array, make image one dimensional, and split provided train and test_data with removing zeros labels at the same time\n",
    "# X.shape : (Pixels_in_one_band, patch_row, patch_col, reflectance_band) \n",
    "# Pixels_in_one_band.shape = 145 * 145 = 21025\n",
    "\n",
    "X_train, X_test, y_train, y_test = patch_1dim_split(X, train_data, test_data, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:46:59.004948Z",
     "start_time": "2018-11-18T17:46:58.996442Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if X and y have compatible shapes (X_*.shape[0] === y_*.shape[0])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:00:01.943104Z",
     "start_time": "2018-11-18T17:00:01.056374Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = oversampleWeakClasses(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:47:02.313768Z",
     "start_time": "2018-11-18T17:46:59.009077Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Add samples. Warning: 33.3s for 100 samples on my PC if no dimensionality reduction. There's room for optimization here. \n",
    "# Optimization Tip: Array is recreated at each iteration.\n",
    "X_train, y_train = BoostDataset(X_train, y_train, n_samples=100)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:47:02.320616Z",
     "start_time": "2018-11-18T17:47:02.316331Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[3], X_train.shape[1], X_train.shape[2]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[3], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=16)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:47:02.359795Z",
     "start_time": "2018-11-18T17:47:02.324197Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "input_shape= X_train[0].shape\n",
    "print(input_shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:01.649862Z",
     "start_time": "2018-11-18T17:47:58.323328Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequential_cnn_model(input_shape, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(C1, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(3*C1, (1,1), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30*numPCAcomponents, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "sequential_cnn_model = sequential_cnn_model(input_shape)\n",
    "\n",
    "# On a 9 classes en réalité, donc une accuracy > 100/9 ~=11.11 est supérieure au hasard. \n",
    "sequential_cnn_model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:04.086421Z",
     "start_time": "2018-11-18T17:48:04.034793Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def nin_cnn():\n",
    "    model = Sequential()\n",
    "    #mlpconv block 1\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu',padding='valid', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((1,1), dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block2\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((1,1), dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block3\n",
    "    model.add(Conv2D(128, (1, 1), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(16, (1, 1)))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation(activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "nin_cnn = nin_cnn()\n",
    "nin_cnn.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:07.599619Z",
     "start_time": "2018-11-18T17:48:06.921262Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_error(model):\n",
    "    pred = model.predict(X_test, batch_size = 32)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "    error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]    \n",
    "    return error\n",
    "\n",
    "def ensemble(models, model_input):\n",
    "    \n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Average()(outputs)\n",
    "    model = Model(model_input, y, name='ensemble')\n",
    "    \n",
    "    return model\n",
    "\n",
    "#models = [nin_cnn, sequential_cnn_model]\n",
    "\n",
    "#ensemble_model = ensemble(models, input_shape)\n",
    "\n",
    "print(evaluate_error(sequential_cnn_model))\n",
    "#print(evaluate_error(nin_cnn))\n",
    "model = sequential_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T15:30:26.196422Z",
     "start_time": "2018-11-18T15:30:26.163395Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "y_enc = mlb.fit_transform(y_train)\n",
    "clf2 = LogisticRegression()\n",
    "print(y_enc)\n",
    "voting = VotingClassifier(estimators=[('cnn', model), ('lr', clf2)], weights=[1.0, 1.0])\n",
    "voting.fit(X_train, y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:11.619306Z",
     "start_time": "2018-11-18T17:48:10.531365Z"
    }
   },
   "outputs": [],
   "source": [
    "classification, confusion, Test_loss, Test_accuracy = reports(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:48:11.671792Z",
     "start_time": "2018-11-18T17:48:11.667106Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:39:10.473656Z",
     "start_time": "2018-11-18T17:39:10.469182Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Test_loss)\n",
    "print(Test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:39:12.836077Z",
     "start_time": "2018-11-18T17:39:12.763198Z"
    }
   },
   "outputs": [],
   "source": [
    "X_garbage, train_data, test_data = loadData()\n",
    "#X = dimensionalityReduction(X, )\n",
    "y = np.add(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:39:48.609630Z",
     "start_time": "2018-11-18T17:39:13.357726Z"
    }
   },
   "outputs": [],
   "source": [
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "print(width, height)\n",
    "\n",
    "# calculate the predicted image\n",
    "outputs = np.zeros((height,width)) # zeroed image\n",
    "for i in range(0, height-PATCH_SIZE+1):\n",
    "    for j in range(0, width-PATCH_SIZE+1):\n",
    "        target = int(y[int(i+PATCH_SIZE/2)][int(j+PATCH_SIZE/2)])\n",
    "        if target == 0 :\n",
    "            continue\n",
    "        else :\n",
    "            image_patch=Patch(X,i,j)\n",
    "            #print (image_patch.shape)\n",
    "            X_test_image = image_patch.reshape(1,image_patch.shape[2],image_patch.shape[0],image_patch.shape[1]).astype('float32')                                   \n",
    "            prediction = (model.predict_classes(X_test_image))                         \n",
    "            outputs[int(i+PATCH_SIZE/2)][int(j+PATCH_SIZE/2)] = prediction + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:39:49.072760Z",
     "start_time": "2018-11-18T17:39:48.700362Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))\n",
    "print(np.unique(y))\n",
    "\n",
    "labelPatches = [ patches.Patch(color=spy_colors[x]/255.,\n",
    "                 label=label_dictionary[x]) for x in np.unique(y) ]\n",
    "plt.legend(handles=labelPatches, ncol=2, fontsize='medium', \n",
    "           loc='upper center', bbox_to_anchor=(0.5, -0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T17:39:49.283323Z",
     "start_time": "2018-11-18T17:39:49.143463Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
